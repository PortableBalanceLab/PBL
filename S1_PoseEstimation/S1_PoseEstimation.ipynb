{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d6bf27",
   "metadata": {},
   "source": [
    "# `S1`: Sensor Lab 1: Pose Estimation\n",
    "\n",
    "Pose estimation refers to computer vision techniques that detect human figures in images and videos, so that one could determine, for example, where someone‚Äôs elbow shows up in an image. It is important to be aware of the fact that pose estimation merely estimates where key body joints are and does not recognize who is in an image or video.\n",
    "\n",
    "In this lab we will be working with the [Raspberry Pi 4](https://www.raspberrypi.com/products/raspberry-pi-4-model-b/), the [Pi Camera](https://projects.raspberrypi.org/en/projects/getting-started-with-picamera), and a [Coral USB Accelerator](https://coral.ai/products/accelerator/).\n",
    "\n",
    "## Outline\n",
    "\n",
    "* [1. Setup Hardware](#Ch1)\n",
    "  * [1.1 Connect the Camera Module](#Ch11)\n",
    "  * [1.2 Connect the USB Coral Accelerator](#Ch12)\n",
    "  * [1.3 Power Up the Pi](#Ch13)\n",
    "* [2. Setup Software](#Ch2)\n",
    "* [3. Try to Control the Camera with Python Code](#Ch3)\n",
    "* [4. Create a Capture Booth GUI to Register Participants' Body Pictures](#Ch4)\n",
    "* [5. Capture Participants' Poses](#Ch5)\n",
    "  * [5.1 Classification](#Ch51)\n",
    "    * [5.1.1 Check if tensorflow works for image classification](#Ch511)\n",
    "    * [5.1.2 Check if the USB coral accelerator works for image classification](#Ch512)\n",
    "  * [5.2 PoseNet](#Ch52)\n",
    "    * [5.2.1 How does it work?](#Ch521)\n",
    "    * [5.2.2 Important PoseNet Concepts](#Ch522)\n",
    "    * [5.2.3 Example PoseNet Code](#Ch523)\n",
    "  * [5.3 Save Pose Data to a CSV](#Ch53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50c1b3",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup Hardware <a id=\"Ch1\"></a>\n",
    "\n",
    "\n",
    "### 1.1 Connect the Camera Module <a id=\"Ch11\"></a>\n",
    "\n",
    "<div>\n",
    "<img src=\"images/Camera_and_pi_4.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "Ensure your Raspberry Pi is turned off.\n",
    "\n",
    "1. Locate the Camera Module port\n",
    "\n",
    "<div>\n",
    "<img src=\"images/pi4-camera-port.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "2. Gently pull up on the edges of the port‚Äôs plastic clip\n",
    "\n",
    "<div>\n",
    "<img src=\"images/pull_edges.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "3. Insert the Camera Module ribbon cable; make sure the connectors at the bottom of the ribbon cable are facing the contacts in the port\n",
    "\n",
    "<div>\n",
    "<img src=\"images/facing_backwards.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "4. Push the plastic clip back into place\n",
    "\n",
    "\n",
    "### 1.2 Connect the USB Coral Accelerator <a id=\"Ch12\" />\n",
    "\n",
    "1. Make sure your RP is switched off.\n",
    "2. Plug the USB Coral Accelerator dongle into a **blue** (USB3) USB slot\n",
    "3. **Note**: the <span style=\"color:#0000ff\">blue</span> USB ports are faster than the not-blue ones\n",
    "\n",
    "\n",
    "### 1.3 Power Up the Pi  <a id=\"Ch13\" />\n",
    "\n",
    "1. Connect the USB-C _charger_ to the Pi\n",
    "2. The Pi will automatically switch on as soon as it has power\n",
    "3. You should now be able to connect to the Pi via VNC (if you are not connected with a physical external screen).\n",
    "\n",
    "\n",
    "## 2. Setup Software  <a id=\"Ch2\" />\n",
    "\n",
    "The lab organizers have already gone through [X0_SoftwareSetup](../X0_SoftwareSetup/README.md), which sets up the necessary software for you, before you were given the Pi, so you probably don't need to set up any software.\n",
    "\n",
    "The configuration script does things like enabling the Pi camera interface and installing `guizero`. If you're curious about what it did, you can read through `s1.py`'s source code in the [pbl](../X0_SoftwareSetup/pbl/pbl) module.\n",
    "\n",
    "> ‚ÑπÔ∏è **Problem With Your Pi?**\n",
    ">\n",
    "> The course organizers have tried their best to ensure all the configuration options and software you'll need is already installed before the course begins, but we can miss things. If you find that the Pi isn't working for you then you can try:\n",
    ">\n",
    "> - Asking for help\n",
    "> - Running `pbl test` in the terminal, which runs some basic checks that ensure things like libraries etc. are installed\n",
    "> - Reinstalling the necessary software by running `sudo pbl install` in the terminal (‚ö†Ô∏è **warning**: takes a long time)\n",
    "> - Manually going through the legacy setup guide [here](Legacy/S1_LegacySoftwareSetup.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f981b0c",
   "metadata": {},
   "source": [
    "## 3. Try to Control the Camera with Python Code <a id=\"Ch3\" />\n",
    "\n",
    "The Python `picamera` library allows you to control your Camera Module. \n",
    "\n",
    "- Open a new file in the editor (e.g. in Mu) and save it as `camera_example.py`. __‚ö†Ô∏è Warning:__ never save the file as `picamera.py`!\n",
    "- Try the following code on your Pi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from picamera import PiCamera\n",
    "from time import sleep\n",
    "\n",
    "camera = PiCamera()\n",
    "\n",
    "camera.start_preview()\n",
    "sleep(5)\n",
    "camera.stop_preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49447e2c",
   "metadata": {},
   "source": [
    "Save and run this program. The camera preview should be shown for five seconds and then close again. \n",
    "\n",
    "> ‚ÑπÔ∏è **Note**: the camera preview only works when a monitor is directly connected to your Raspberry Pi. If you are using remote access (such as SSH or VNC), you won‚Äôt be able to see the camera preview. You can work around this by saving an image and viewing that instead (the next steps of this lab).\n",
    "\n",
    "> ‚ùì **Test Yourselves**: Try to describe line-by-line what this python code is doing.\n",
    "\n",
    "<br />\n",
    "\n",
    "> üèÜ **Challenge**: Save a picture from the camera by using the `camera.capture()` function. Save the image as `capture.jpg` in `/home/pi/Desktop`\n",
    ">\n",
    "> (note: it‚Äôs important to sleep for at least two seconds _before_ capturing an image, to give the camera time to adjust to the room's light levels.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b233702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your own code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252e6e5",
   "metadata": {},
   "source": [
    "If your picture is upside-down, you can rotate it by 180 degrees by adding the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b696d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.rotation = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc2310",
   "metadata": {},
   "source": [
    "You can rotate the image by 90, 180, or 270 degrees. To reset the image, set `camera.rotation` to 0 degrees.\n",
    "\n",
    "The Python `picamera` software provides a number of effects and configurations to change how your images look. Check out the following website to find some examples:\n",
    "https://projects.raspberrypi.org/en/projects/getting-started-with-picamera/7\n",
    "\n",
    "All documentation on the PiCamera project can be found here:\n",
    "https://picamera.readthedocs.io/en/release-1.13/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60beb2",
   "metadata": {},
   "source": [
    "## 4. Create a Capture Booth GUI to Register Participants' Body Pictures <a id=\"Ch4\" />\n",
    "\n",
    "Now that you have gotten to know the picamera a bit better, you will now make a simple GUI that you can use to capture pictures of your participants' (clothed üòâ) bodies.\n",
    "\n",
    "> üèÜ **Challenge**: Use `guizero` (see [L3](../L3_PythonGUIsAndHardware/L3_PythonGUIsAndHardware.ipynb)) to create a capture booth GUI.\n",
    ">\n",
    "> - It should request a participant ID (e.g. via a text box in which you type an ID like  `P01`)\n",
    "> - It should have a button that, when pressed, causes the application to take a picture of the participant's body\n",
    "> - The picture should be saved as a file with a relevant name (e.g. `P01_front.png`) in a folder called `participants`\n",
    "> - It should show the captured picture in the GUI\n",
    ">\n",
    "> ‚ÑπÔ∏è **Note**:\n",
    ">\n",
    "> - The face of the participant should not be shown in the picture. Make sure that the camera only captures a picture of the body by verbally instructing the participant on where to stand in front of the camera (or move the camera around).\n",
    ">\n",
    "> üí° **Tips**:\n",
    ">\n",
    "> - Start by creating your GUI layout without actually implementing the functionality. E.g. use a placeholder image where the participant's image will ultimately go, and later substitute it for the actual picture.\n",
    "> - Some example code is already provided below to help you get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68282b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code: modify this for your requirements\n",
    "#\n",
    "# note: you don't have to write your code in this way, this is just\n",
    "#       a suggestion.\n",
    "\n",
    "from picamera import PiCamera\n",
    "from time import sleep\n",
    "from guizero import *\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "# a class that encapsulates the GUI code\n",
    "class MocapGUI:\n",
    "\n",
    "    # code for initializing the GUI\n",
    "    def __init__(self):\n",
    "\n",
    "        self.app = App(title=\"Capture Booth\", width=800, height=400)\n",
    "\n",
    "        if not os.path.isdir(\"participants\"): # these lines make sure that the folder for saving the measurements exists\n",
    "            os.mkdir(\"participants\")\n",
    "\n",
    "        # the GUI should have 3 boxes:\n",
    "            # `box1` is only for aesthetic purposes and should not be used\n",
    "            # `box2` should contain:\n",
    "                # 1. a list displaying the participant IDs from which to pick the right ID for the measurements\n",
    "                    # This list should be updated when a new participant is registered\n",
    "                # 2. A button for adding a new participant ID (Optional: and saving it to a file containing all the IDs)\n",
    "                # 3. A button for capturing a picture with the camera\n",
    "            # `box3` will be used to display the captured image\n",
    "\n",
    "        self.box1 = Box(self.app, align =\"top\", width=300, height=50)\n",
    "        self.box2 = Box(self.app, align =\"left\", layout=\"auto\", width=300, height=350)\n",
    "        self.box3 = Box(self.app, align =\"right\", width=500, height=350)\n",
    "\n",
    "        # write your own code here for adding other widgets\n",
    "\n",
    "    # code for displaying the GUI\n",
    "    def display(self):\n",
    "        self.app.display()\n",
    "\n",
    "    # code for registering a new participant ID\n",
    "    def ask_id(self):\n",
    "        pass  # (write your own code here)\n",
    "\n",
    "    # code for selecting a participant ID\n",
    "    def select_id(self):\n",
    "        pass # (write your own code here)\n",
    "\n",
    "    # code for capturing an image using the Raspberry Pi camera\n",
    "    def capture(self):\n",
    "        pass # (write your own code here)\n",
    "\n",
    "        \n",
    "# optional: a class that handles a CSV file that stores the participant IDs\n",
    "class CsvIDHandler:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # code for reading IDs from a CSV file located at `fname`\n",
    "    #\n",
    "    # should return a list of the participant IDs that were stored in `fname`\n",
    "    def read_ids(self, fname):\n",
    "        if not self.already_file(fname):\n",
    "            self.create_file(fname)\n",
    "\n",
    "        # (write your own code here)\n",
    "        #\n",
    "        # it should read IDs from `fname` and store them in a list that is\n",
    "        # returned to the caller\n",
    "        content = []\n",
    "        return content\n",
    "\n",
    "    # code for writing a newly added participant ID to the file\n",
    "    def write_id(self, fname, content):\n",
    "        if not self.already_file(fname):\n",
    "            self.create_file(fname)\n",
    "\n",
    "        # (write your own code here)\n",
    "        #\n",
    "        # it should append the `content` (ID) to the end of the file\n",
    "\n",
    "    # code for checking if `fname` exists\n",
    "    def already_file(self, fname):\n",
    "        return os.path.isfile(fname)\n",
    "\n",
    "    # code for creating a new ID file\n",
    "    def create_file(self, fname):\n",
    "        f = open(fname, 'x', newline='')\n",
    "        f.close()\n",
    "\n",
    "# this part of the code actually runs the GUI\n",
    "gui = MocapGUI()\n",
    "gui.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0fe52",
   "metadata": {},
   "source": [
    "## 5. Capture Participants' Poses <a id=\"Ch5\"></a>\n",
    "\n",
    "Pose estimation is the task of using an Machine Learning model to estimate the pose of a person from an image or a video by estimating the spatial locations of key body joints (keypoints). In this lab you are going to set up a __portable pose estimation lab__ using the __Raspberry Pi 4__, the __Pi Camera__, and the __Coral USB Accelerator__.\n",
    "\n",
    "The __Coral USB Accelerator__ is a USB device that provides an __Edge TPU__ as a coprocessor for your device. It accelerates inferencing for your machine learning models when attached to either a Linux, Mac, or Windows host computer. \n",
    "\n",
    "The pose estimation models take a processed camera image as the input and outputs information about keypoints. The keypoints detected are indexed by a part ID, with a confidence score between 0.0 and 1.0. The confidence score indicates the probability that a keypoint exists in that position.\n",
    "\n",
    "There are two TensorFlow Lite pose estimation models:\n",
    "- MoveNet: the state-of-the-art pose estimation model available in two flavors: Lighting and Thunder. \n",
    "- PoseNet: the previous generation pose estimation model released in 2017.\n",
    "\n",
    "In this lab you'll work with the __PoseNet__ model.\n",
    "\n",
    "The various body joints detected by the pose estimation model are tabulated below:\n",
    "\n",
    "\n",
    "| Id | Part |\n",
    "| --- | --- |\n",
    "|0|\tnose |\n",
    "|1|\tleftEye |\n",
    "|2|\trightEye |\n",
    "|3|\tleftEar |\n",
    "|4|\trightEar |\n",
    "|5|\tleftShoulder|\n",
    "|6|\trightShoulder|\n",
    "|7|\tleftElbow|\n",
    "|8|\trightElbow|\n",
    "|9|\tleftWrist|\n",
    "|10|\trightWrist|\n",
    "|11\t| leftHip|\n",
    "|12\t| rightHip|\n",
    "|13\t| leftKnee|\n",
    "|14\t| rightKnee|\n",
    "|15|\tleftAnkle|\n",
    "|16\t| rightAnkle|\n",
    "\n",
    "\n",
    "\n",
    "![PoseNetExample](images/PoseNet_example.png)\n",
    "source: https://www.tensorflow.org/lite/examples/pose_estimation/overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48715632",
   "metadata": {},
   "source": [
    "### 5.1 Classification <a class=\"anchor\" id=\"Ch51\"></a>\n",
    "\n",
    "We will first run general examples to check if the required libraries are installed. In this example we are going to classify the following image:\n",
    "\n",
    "<div>\n",
    "<img src=\"images/parrot.jpg\" width=\"200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2740790",
   "metadata": {},
   "source": [
    "#### 5.1.1 Check if tensorflow works for image classification <a id=\"Ch511\" />\n",
    "\n",
    "First we want to navigate to the folder containing our test, do this by typing in the Terminal:\n",
    "\n",
    "```bash\n",
    "cd /opt/coral_example\n",
    "```\n",
    "    \n",
    "Than we will try classify an example image with a tensorflow model, do this by copy-pasting the following code and press enter:\n",
    "\n",
    "```bash\n",
    "python3 classify_image.py \\\n",
    "  --model mobilenet_v2_1.0_224_inat_bird_quant.tflite \\\n",
    "  --labels inat_bird_labels.txt \\\n",
    "  --input parrot.jpg\n",
    "```\n",
    "\n",
    "You should get something like this:\n",
    "\n",
    "```text\n",
    "----INFERENCE TIME----\n",
    "Note: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.\n",
    "317.5ms\n",
    "288.7ms\n",
    "286.4ms\n",
    "286.4ms\n",
    "286.5ms\n",
    "-------RESULTS--------\n",
    "Ara macao (Scarlet Macaw): 0.77734\n",
    "```\n",
    "\n",
    "#### 5.1.2 Check if the USB coral accelerator works for image classification <a id=\"Ch512\" />\n",
    "\n",
    "Now we will check the availability of the USB coral accelerator by running the same test again. This time we use a model that is compiled to run on the USB tpu.\n",
    "\n",
    "Copy-paste the following code and run by pressing enter:\n",
    "\n",
    "```bash\n",
    "python3 classify_image.py \\\n",
    "  --model mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite \\\n",
    "  --labels inat_bird_labels.txt \\\n",
    "  --input parrot.jpg\n",
    "```\n",
    "    \n",
    "You should see a result like this:\n",
    "\n",
    "```text\n",
    "----INFERENCE TIME----\n",
    "Note: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.\n",
    "11.8ms\n",
    "3.0ms\n",
    "2.8ms\n",
    "2.9ms\n",
    "2.9ms\n",
    "-------RESULTS--------\n",
    "Ara macao (Scarlet Macaw): 0.75781\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8c8c8",
   "metadata": {},
   "source": [
    "### 5.2 PoseNet <a id=\"Ch52\" />\n",
    "\n",
    "(https://github.com/google-coral/project-posenet)\n",
    "\n",
    "#### 5.2.1 How does it work? <a id=\"Ch521\" />\n",
    "\n",
    "At a high level, pose estimation happens in two phases:\n",
    "\n",
    "An input RGB image is fed through a convolutional neural network. In our case this is a MobileNet V1 architecture. Instead of a classification head however, there is a specialized head which produces a set of heatmaps (one for each kind of key point) and some offset maps. This step runs on the EdgeTPU. The results are then fed into step 2)\n",
    "\n",
    "- A special multi-pose decoding algorithm is used to decode poses, pose confidence scores, keypoint positions, and keypoint confidence scores. \n",
    "\n",
    "- If you're interested in the details of the decoding algorithm and how PoseNet works under the hood, you could take a look at the original research paper or this post: https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5 which describes the raw heatmaps produced by the convolutional model.\n",
    "\n",
    "#### 5.2.2 Important PoseNet Concepts <a id=\"Ch522\" />\n",
    "\n",
    "<div> <img src=\"images/keypoints.png\" width=\"800\"></div>\n",
    "\n",
    "| Concept | Description |\n",
    "| ------- | ----------- |\n",
    "| Pose    | At the highest level, PoseNet will return a pose object that contains a list of keypoints and an instance-level confidence score for each detected person. |\n",
    "| Keypoint | A part of a person‚Äôs pose that is estimated, such as the nose, right ear, left knee, right foot, etc. It contains both a position and a keypoint confidence score. PoseNet currently detects 17 keypoints illustrated in the diagram above.\n",
    "| Keypoint Confidence Score | This determines the confidence that an estimated keypoint position is accurate. It ranges between 0.0 and 1.0. It can be used to hide keypoints that are not deemed strong enough. |\n",
    "| Keypoint Position | 2D x and y coordinates in the original input image where a keypoint has been detected. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac0893",
   "metadata": {},
   "source": [
    "#### 5.2.3 Example PoseNet Code <a id=\"Ch523\" />\n",
    "\n",
    "> **Note**: PoseNet should already have been installed for you\n",
    ">\n",
    "> This is explained in the Software Setup section, but if you are having issues then you can also try to manually go through the legacy notes [here](Legacy/S1_LegacySoftwareSetup.ipynb)\n",
    "\n",
    "The example code, `pose_camera.py`, is a camera example that streams the camera's image through posenet and draws the pose on top as an overlay. This is a great first example to run to familiarize yourself with the network and its outputs.\n",
    "\n",
    "Run the demo in a terminal:\n",
    "\n",
    "```bash\n",
    "cd /opt/project-posenet\n",
    "python3 pose_camera.py\n",
    "```\n",
    "\n",
    "If the camera and monitor are both facing you, consider adding the `--mirror` flag:\n",
    "\n",
    "```bash\n",
    "python3 pose_camera.py --mirror\n",
    "```\n",
    "\n",
    "> ‚ÑπÔ∏è **Note**: The github repository (https://github.com/google-coral/project-posenet.git) contains the following 3 posenet model files in `models/mobilenet` for different input resolutions. The larger resolutions process more slowly, but allow a wider field of view, for further-away poses to be processed correctly.\n",
    ">\n",
    "> ```text\n",
    "> posenet_mobilenet_v1_075_721_1281_quant_decoder_edgetpu.tflite\n",
    "> posenet_mobilenet_v1_075_481_641_quant_decoder_edgetpu.tflite\n",
    "> posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite\n",
    "> ```\n",
    "\n",
    "You can change the camera resolution by using the --res parameter:\n",
    "\n",
    "```bash\n",
    "python3 pose_camera.py --res 480x360  # fast but low res\n",
    "python3 pose_camera.py --res 640x480  # default\n",
    "python3 pose_camera.py --res 1280x720 # slower but high res\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6463c43",
   "metadata": {},
   "source": [
    "### 5.3 Save Pose Data to a CSV <a id=\"Ch53\" />\n",
    "\n",
    "In the previous section you have extracted keypoints from a live video using PoseNet. However, we cannot analyse data if this isn't saved somewhere.\n",
    "\n",
    "To do this, you are going to need to know how to generate unique timestamped filenames ([X2](../X2_GeneratingTimestampedFilenames/X2_GeneratingTimestampedFilenames.ipynb)) and how to write to CSV files ([X1](../X1_WritingCSVFiles/X1_WritingCSVFiles.ipynb)). You should then produce a modified version of `pose_camera.py` called `logging_pose_camera.py` that logs (writes) your keypoints to a data file.\n",
    "\n",
    "> üèÜ **Challenge**: Go through the [X1](../X1_WritingCSVFiles/X1_WritingCSVFiles.ipynb) and [X2](../X2_GeneratingTimestampedFilenames/X2_GeneratingTimestampedFilenames.ipynb) \"eXtra Content\" materials and write `logging_pose_camera.py`, which should log poses to a timestamped CSV file.\n",
    ">\n",
    "> - After going through [X1](../X1_WritingCSVFiles/X1_WritingCSVFiles.ipynb), you should know how to write CSV files\n",
    "> - After going through [X2](../X2_GeneratingTimestampedFilenames/X2_GeneratingTimestampedFilenames.ipynb), you should know how to generate timestamped file names\n",
    "> - Make a copy of `/opt/project-posenet` on your desktop with `cp -ar /opt/project-posenet ~/Desktop`\n",
    "> - Copy `pose_camera.py` in your desktop copy to a different name: `logging_pose_camera.py` (i.e. your logging version of `pose_camera.py`)\n",
    "> - Verify that you can run the unmodified, but renamed, version of the python file by first switching to the copied directory (`cd ~/Desktop/project-posenet`) and running the script (`python logging_pose_camera.py`)\n",
    "> - Combine X1 and X2 techniques and edit `logging_pose_camera.py` such that it writes your keypoints to a timestamped CSV file. The file should be saved as `data/output_yourdatestring.csv`.\n",
    "> - Make sure to close your CSV file at the end of the program/acquisition\n",
    ">\n",
    "> üí° **Tips**:\n",
    ">\n",
    "> - You only need one file (via `open`) and one `csv.writer` for the entire acquisition.\n",
    "> - `main` is only called once per acquisition.\n",
    "> - You will need to insert multiple rows during an acquisition - one per frame. \n",
    "> - `render_overlay` is called once per frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ede698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí° tip: you may need to build a row of your CSV cell-by-cell\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "row = []\n",
    "row.append(datetime.now())  # append a timestamp in the first column \n",
    "for label, keypoint in pose.keypoints.items(): # loop through all keypoints\n",
    "    row.append(keypoint.point[0]) # append the x-coordinate of the keypoint to the row \n",
    "    row.append(keypoint.point[1]) # append the y-coordinate of the keypoint in the row\n",
    "\n",
    "# (and then you need to write this row to a CSV using a `csv.writer`: see X1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
