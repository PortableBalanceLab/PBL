{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d6bf27",
   "metadata": {},
   "source": [
    "# `S1`: Sensor Lab 1: Pose Estimation\n",
    "\n",
    "Pose estimation refers to computer vision techniques that detect human figures in images and videos, so that one could determine, for example, where someone‚Äôs elbow shows up in an image. It is important to be aware of the fact that pose estimation merely estimates where key body joints are and does not recognize who is in an image or video.\n",
    "\n",
    "In this lab we will be working with the [Raspberry Pi 4](https://www.raspberrypi.com/products/raspberry-pi-4-model-b/), the [Pi Camera](https://projects.raspberrypi.org/en/projects/getting-started-with-picamera), and a [Coral USB Accelerator](https://coral.ai/products/accelerator/).\n",
    "\n",
    "__Outline:__\n",
    "* [1. Connect the Camera Module](#Ch1)\n",
    "* [2. Control the Camera with Python Code](#Ch2)\n",
    "* [3. Creating a Capture Booth to Register participants' pictures](#Ch3)\n",
    "* [4.  Pose estimation](#Ch4)\n",
    "    * [4.1. Classification](#Ch41)\n",
    "    * [4.2. PoseNet](#Ch42)\n",
    "    * [4.3. Save data to a file](#Ch43)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50c1b3",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Connect the Camera Module <a class=\"anchor\" id=\"Ch1\"></a>\n",
    "\n",
    "<div>\n",
    "<img src=\"images/Camera_and_pi_4.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "Ensure your Raspberry Pi is turned off.\n",
    "1. Locate the Camera Module port\n",
    "<div>\n",
    "<img src=\"images/pi4-camera-port.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "2. Gently pull up on the edges of the port‚Äôs plastic clip\n",
    "<div>\n",
    "<img src=\"images/pull_edges.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "3. Insert the Camera Module ribbon cable; make sure the connectors at the bottom of the ribbon cable are facing the contacts in the port\n",
    "<div>\n",
    "<img src=\"images/facing_backwards.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "4. Push the plastic clip back into place\n",
    "\n",
    "5. Start up your Raspberry Pi\n",
    "6. Go to the main menu and open the __Raspberry Pi Configuration__ tool.\n",
    "7. Select the __Interfaces__ tab and ensure that the __camera__ is __enabled__\n",
    "8. Reboot your Raspberry Pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f981b0c",
   "metadata": {},
   "source": [
    "## 2. Try to Control the Camera with Python Code <a class=\"anchor\" id=\"Ch2\"></a>\n",
    "\n",
    "The Python `picamera` library allows you to control your Camera Module. \n",
    "\n",
    "- Open a new file in the editor (e.g. in Mu) and save it as `camera_example.py`. __‚ö†Ô∏è Warning:__ never save the file as `picamera.py`!\n",
    "- Try the following code on your Pi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from picamera import PiCamera\n",
    "from time import sleep\n",
    "\n",
    "camera = PiCamera()\n",
    "\n",
    "camera.start_preview()\n",
    "sleep(5)\n",
    "camera.stop_preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49447e2c",
   "metadata": {},
   "source": [
    "Save and run this program. The camera preview should be shown for five seconds and then close again. \n",
    "\n",
    "> ‚ÑπÔ∏è **Note**: the camera preview only works when a monitor is directly connected to your Raspberry Pi. If you are using remote access (such as SSH or VNC), you won‚Äôt be able to see the camera preview. You can work around this by saving an image and viewing that instead (the next steps of this lab).\n",
    "\n",
    "> ‚ùì **Test Yourselves**: Try to describe line-by-line what this python code is doing.\n",
    "\n",
    "<br />\n",
    "\n",
    "> üèÜ **Challenge**: Save a picture from the camera by using the `camera.capture()` function. Save the image as `capture.jpg` in `/home/pi/Desktop`\n",
    ">\n",
    "> (note: it‚Äôs important to sleep for at least two seconds _before_ capturing an image, to give the camera time to adjust to the room's light levels.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b233702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your own code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252e6e5",
   "metadata": {},
   "source": [
    "If your picture is upside-down, you can rotate it by 180 degrees by adding the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b696d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.rotation = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc2310",
   "metadata": {},
   "source": [
    "You can rotate the image by 90, 180, or 270 degrees. To reset the image, set `camera.rotation` to 0 degrees.\n",
    "\n",
    "The Python `picamera` software provides a number of effects and configurations to change how your images look. Check out the following website to find some examples:\n",
    "https://projects.raspberrypi.org/en/projects/getting-started-with-picamera/7\n",
    "\n",
    "All documentation on the PiCamera project can be found here:\n",
    "https://picamera.readthedocs.io/en/release-1.13/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60beb2",
   "metadata": {},
   "source": [
    "## 3. Create a Capture Booth GUI to Register Participants' Body Pictures <a class=\"anchor\" id=\"Ch3\"></a>\n",
    "\n",
    "Now that you have gotten to know the picamera a bit better, you will now make a simple GUI that you can use to capture pictures of your participants' (clothed üòâ) bodies.\n",
    "\n",
    "> üèÜ **Challenge**: Use `guizero` (see [L3](../L3_PythonGUIsAndHardware/L3_PythonGUIsAndHardware.ipynb)) to create a capture booth GUI.\n",
    ">\n",
    "> - It should request a participant ID (e.g. via a text box in which you type an ID like  `P01`)\n",
    "> - It should have a button that, when pressed, causes the application to take a picture of the participant's body\n",
    "> - The picture should be saved as a file with a relevant name (e.g. `P01_front.png`) in a folder called `participants`\n",
    "> - It should show the captured picture in the GUI\n",
    ">\n",
    "> ‚ÑπÔ∏è **Note**:\n",
    ">\n",
    "> - The face of the participant should not be shown in the picture. Make sure that the camera only captures a picture of the body by verbally instructing the participant on where to stand in front of the camera (or move the camera around).\n",
    ">\n",
    "> üí° **Tips**:\n",
    ">\n",
    "> - Start by creating your GUI layout without actually implementing the functionality. E.g. use a placeholder image where the participant's image will ultimately go, and later substitute it for the actual picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68282b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill out your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0fe52",
   "metadata": {},
   "source": [
    "## 4. Capture Participants' Poses <a class=\"anchor\" id=\"Ch4\"></a>\n",
    "\n",
    "Pose estimation is the task of using an Machine Learning model to estimate the pose of a person from an image or a video by estimating the spatial locations of key body joints (keypoints). In this lab you are going to set up a __portable pose estimation lab__ using the __Raspberry Pi 4__, the __Pi Camera__, and the __Coral USB Accelerator__. The __Coral USB Accelerator__ is a USB device that provides an __Edge TPU__ as a coprocessor for your device. It accelerates inferencing for your machine learning models when attached to either a Linux, Mac, or Windows host computer. \n",
    "\n",
    "The pose estimation models take a processed camera image as the input and outputs information about keypoints. The keypoints detected are indexed by a part ID, with a confidence score between 0.0 and 1.0. The confidence score indicates the probability that a keypoint exists in that position.\n",
    "\n",
    "There are two TensorFlow Lite pose estimation models:\n",
    "- MoveNet: the state-of-the-art pose estimation model available in two flavors: Lighting and Thunder. \n",
    "- PoseNet: the previous generation pose estimation model released in 2017.\n",
    "\n",
    "In this lab you'll work with the __PoseNet__ model.\n",
    "\n",
    "The various body joints detected by the pose estimation model are tabulated below:\n",
    "\n",
    "\n",
    "| Id | Part |\n",
    "| --- | --- |\n",
    "|0|\tnose |\n",
    "|1|\tleftEye |\n",
    "|2|\trightEye |\n",
    "|3|\tleftEar |\n",
    "|4|\trightEar |\n",
    "|5|\tleftShoulder|\n",
    "|6|\trightShoulder|\n",
    "|7|\tleftElbow|\n",
    "|8|\trightElbow|\n",
    "|9|\tleftWrist|\n",
    "|10|\trightWrist|\n",
    "|11\t| leftHip|\n",
    "|12\t| rightHip|\n",
    "|13\t| leftKnee|\n",
    "|14\t| rightKnee|\n",
    "|15|\tleftAnkle|\n",
    "|16\t| rightAnkle|\n",
    "\n",
    "\n",
    "\n",
    "![PoseNetExample](images/PoseNet_example.png)\n",
    "source: https://www.tensorflow.org/lite/examples/pose_estimation/overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48715632",
   "metadata": {},
   "source": [
    "### 4.1 Classification <a class=\"anchor\" id=\"Ch41\"></a>\n",
    "\n",
    "We will first run general examples to  check if the required libraries are installed. In this example we are going to classify the following image:\n",
    "\n",
    "<div>\n",
    "<img src=\"images/parrot.jpg\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "\n",
    "1. Make sure your RP is switched off.\n",
    "2. Connect the RP to:\n",
    "    - Pi Camera\n",
    "    - USB Coral Accelerator\n",
    "3. Now connect the _charger_ to switch on the Raspberry Pi\n",
    "4. Connect with the RP via de VNC Viewer (if you are not connected to an external screen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2740790",
   "metadata": {},
   "source": [
    "#### 4.1.1 Check if tensorflow works for image classification:\n",
    "First we want to navigate to the folder containing our test, do this by typing in the Terminal:\n",
    "\n",
    "```bash\n",
    "cd coral/tflite/python/examples/classification\n",
    "```\n",
    "    \n",
    "Than we will try classify an example image with a tensorflow model, do this by copy-pasting the following code and press enter:\n",
    "\n",
    "```bash\n",
    "python3 classify_image.py \\\n",
    "  --model models/mobilenet_v2_1.0_224_inat_bird_quant.tflite \\\n",
    "  --labels models/inat_bird_labels.txt \\\n",
    "  --input images/parrot.jpg\n",
    "```\n",
    "\n",
    "You should get something like this:\n",
    "\n",
    "```text\n",
    "----INFERENCE TIME----\n",
    "Note: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.\n",
    "317.5ms\n",
    "288.7ms\n",
    "286.4ms\n",
    "286.4ms\n",
    "286.5ms\n",
    "-------RESULTS--------\n",
    "Ara macao (Scarlet Macaw): 0.77734\n",
    "```\n",
    "\n",
    "#### 4.1.2 Check if the USB coral accelerator works for image classification:\n",
    "\n",
    "Now we will check the availability of the USB coral accelerator by running the same test again. This time we use a model that is compiled to run on the USB tpu.\n",
    "\n",
    "Copy-paste the following code and run by pressing enter:\n",
    "\n",
    "```bash\n",
    "python3 classify_image.py \\\n",
    "  --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite \\\n",
    "  --labels models/inat_bird_labels.txt \\\n",
    "  --input images/parrot.jpg\n",
    "```\n",
    "    \n",
    "You should see a result like this:\n",
    "\n",
    "```text\n",
    "----INFERENCE TIME----\n",
    "Note: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.\n",
    "11.8ms\n",
    "3.0ms\n",
    "2.8ms\n",
    "2.9ms\n",
    "2.9ms\n",
    "-------RESULTS--------\n",
    "Ara macao (Scarlet Macaw): 0.75781\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8c8c8",
   "metadata": {},
   "source": [
    "### 4.2 PoseNet <a class=\"anchor\" id=\"Ch42\"></a>\n",
    "\n",
    "(https://github.com/google-coral/project-posenet)\n",
    "\n",
    "#### 4.2.1 How does it work? <a class=\"anchor\" id=\"Ch421\"></a>\n",
    "\n",
    "At a high level, pose estimation happens in two phases:\n",
    "\n",
    "An input RGB image is fed through a convolutional neural network. In our case this is a MobileNet V1 architecture. Instead of a classification head however, there is a specialized head which produces a set of heatmaps (one for each kind of key point) and some offset maps. This step runs on the EdgeTPU. The results are then fed into step 2)\n",
    "\n",
    "- A special multi-pose decoding algorithm is used to decode poses, pose confidence scores, keypoint positions, and keypoint confidence scores. \n",
    "\n",
    "- If you're interested in the details of the decoding algorithm and how PoseNet works under the hood, you could take a look at the original research paper or this post: https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5 which describes the raw heatmaps produced by the convolutional model.\n",
    "\n",
    "#### 4.2.2 Important PoseNet Concepts <a class=\"anchor\" id=\"Ch422\"></a>\n",
    "\n",
    "<div> <img src=\"images/keypoints.png\" width=\"800\"></div>\n",
    "\n",
    "| Concept | Description |\n",
    "| ------- | ----------- |\n",
    "| Pose    | At the highest level, PoseNet will return a pose object that contains a list of keypoints and an instance-level confidence score for each detected person. |\n",
    "| Keypoint | A part of a person‚Äôs pose that is estimated, such as the nose, right ear, left knee, right foot, etc. It contains both a position and a keypoint confidence score. PoseNet currently detects 17 keypoints illustrated in the diagram above.\n",
    "| Keypoint Confidence Score | This determines the confidence that an estimated keypoint position is accurate. It ranges between 0.0 and 1.0. It can be used to hide keypoints that are not deemed strong enough. |\n",
    "| Keypoint Position | 2D x and y coordinates in the original input image where a keypoint has been detected. |\n",
    "\n",
    "\n",
    "#### PoseNet Setup\n",
    "\n",
    "Now that we have verified that TensorFlow and Coral are working, we will now install the required packages for PoseNet. In a terminal window, run the following commands:\n",
    "\n",
    "- Install `python3-pycoral`:\n",
    "\n",
    "```bash\n",
    "sudo apt-get install python3-pycoral\n",
    "```\n",
    "\n",
    "- Install `tflite`:\n",
    "\n",
    "```bash\n",
    "python3 -m pip install tflite-runtime\n",
    "```\n",
    "\n",
    "- Install the Edge TPU runtime:\n",
    "\n",
    "```bash\n",
    "echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "sudo apt-get update  \n",
    "sudo apt-get install libedgetpu1-std\n",
    "```    \n",
    "\n",
    "After installing those, you can now physically connect your __Coral__ to the Raspberry Pi using the USB-C cable supplied. Because the Coral supports USB 3.0, it should be attached to one of the Blue USB ports on the Raspberry Pi 4 to allow the fastest transfer speeds. If you already attached the Coral, then remove it and connect again.\n",
    "\n",
    "```bash     \n",
    "cd ~/google-coral\n",
    "```\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/google-coral/project-posenet.git\n",
    "```\n",
    "    \n",
    "If you receive the message 'fatal: destination path 'project-posenet' already exists and is not an empty directory.' Posenet was already downloaded on your device. Move on to the next step:\n",
    "\n",
    "```bash\n",
    "cd /home/pi/google-coral/project-posenet    \n",
    "sh install_requirements.sh\n",
    "```   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac0893",
   "metadata": {},
   "source": [
    "#### 4.2.3 Example PoseNet Code <a class=\"anchor\" id=\"Ch423\"></a>\n",
    "\n",
    "The example code, `pose_camera.py`, is a camera example that streams the camera's image through posenet and draws the pose on top as an overlay. This is a great first example to run to familiarize yourself with the network and its outputs.\n",
    "\n",
    "Run the demo in a terminal:\n",
    "\n",
    "```bash\n",
    "cd /home/pi/google-coral/project-posenet\n",
    "python3 pose_camera.py\n",
    "```\n",
    "\n",
    "If the camera and monitor are both facing you, consider adding the --mirror flag:\n",
    "\n",
    "```bash\n",
    "python3 pose_camera.py --mirror\n",
    "```\n",
    "\n",
    "> ‚ÑπÔ∏è **Note**: The github repository contains the following 3 posenet model files for different input resolutions. The larger resolutions process more slowly, but allow a wider field of view, for further-away poses to be processed correctly.\n",
    ">\n",
    "> ```text\n",
    "> posenet_mobilenet_v1_075_721_1281_quant_decoder_edgetpu.tflite\n",
    "> posenet_mobilenet_v1_075_481_641_quant_decoder_edgetpu.tflite\n",
    "> posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite\n",
    "> ```\n",
    "\n",
    "You can change the camera resolution by using the --res parameter:\n",
    "\n",
    "```bash\n",
    "python3 pose_camera.py --res 480x360  # fast but low res\n",
    "python3 pose_camera.py --res 640x480  # default\n",
    "python3 pose_camera.py --res 1280x720 # slower but high res\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6463c43",
   "metadata": {},
   "source": [
    "### 4.3 Save Pose Data to a CSV <a class=\"anchor\" id=\"Ch43\"></a>\n",
    "\n",
    "In the previous section you have extracted keypoints from a live video using OpenPose. However, we cannot analyse data if this isn't saved somewhere.\n",
    "\n",
    "To do this, you are going to need to know how to generate unique timestamped filenames ([X2](../X2_GeneratingTimestampedFilenames/X2_GeneratingTimestampedFilenames.ipynb)) and how to write to CSV files ([X1](../X1_WritingCSVFiles/X1_WritingCSVFiles.ipynb)). You should then produce a modified version of `pose_camera.py` called `logging_pose_camera.py` that logs (writes) your keypoints to a data file.\n",
    "\n",
    "> üèÜ **Challenge**: Go through the [X1](../X1_WritingCSVFiles/X1_WritingCSVFiles.ipynb) and [X2](../X2_GeneratingTimestampedFilenames/X2_GeneratingTimestampedFilenames.ipynb) \"eXtra Content\" materials.\n",
    ">\n",
    "> - After going through [X1](../X1_WritingCSVFiles/X1_WritingCSVFiles.ipynb), you should know how to write CSV files\n",
    "> - After going through [X2](../X2_GeneratingTimestampedFilenames/X2_GeneratingTimestampedFilenames.ipynb), you should know how to generate timestamped file names\n",
    "> - Save `pose_camera.py` under a different name: `logging_pose_camera.py` (i.e. a logging version of `pose_camera.py`)\n",
    "> - Combine both techniques and edit `logging_pose_camera.py` such that it writes your keypoints to a timestamped CSV file. The file should be saved as `data/output_yourdatestring.csv`.\n",
    "> - Make sure to close your CSV file at the end of the program/acquisition\n",
    ">\n",
    "> üí° **Tips**:\n",
    ">\n",
    "> - You only need one file (via `open`) and one `csv.writer` for the entire acquisition.\n",
    "> - `main` is only called once per acquisition.\n",
    "> - You will need to insert multiple rows during an acquisition - one per frame. \n",
    "> - `render_overlay` is called once per frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ede698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí° tip: you may need to build a row of your CSV cell-by-cell\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "row = []\n",
    "row.append(datetime.now())  # append a timestamp in the first column \n",
    "for label, keypoint in pose.keypoints.items(): # loop through all keypoints\n",
    "    row.append(keypoint.point[0]) # append the x-coordinate of the keypoint to the row \n",
    "    row.append(keypoint.point[1]) # append the y-coordinate of the keypoint in the row\n",
    "\n",
    "# (and then you need to write this row to a CSV using a `csv.writer`: see X1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
