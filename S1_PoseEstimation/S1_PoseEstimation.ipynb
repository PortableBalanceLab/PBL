{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d6bf27",
   "metadata": {},
   "source": [
    "# `S1`: Sensor Lab 1: Pose Estimation\n",
    "\n",
    "Pose estimation refers to computer vision techniques that detect human figures in images and videos, so that one could determine, for example, where someone‚Äôs elbow shows up in an image. It is important to be aware of the fact that pose estimation merely estimates where key body joints are and does not recognize who is in an image or video.\n",
    "\n",
    "In this lab we will be working with the [Raspberry Pi 4](https://www.raspberrypi.com/products/raspberry-pi-4-model-b/), the [Pi Camera](https://projects.raspberrypi.org/en/projects/getting-started-with-picamera), and a [Coral USB Accelerator](https://coral.ai/products/accelerator/).\n",
    "\n",
    "## Outline\n",
    "\n",
    "* [1. Setup Hardware](#Ch1)\n",
    "  * [1.1 Connect the Camera Module](#Ch11)\n",
    "  * [1.2 Connect the USB Coral Accelerator](#Ch12)\n",
    "  * [1.3 Power Up the Pi](#Ch13)\n",
    "* [2. Setup Software](#Ch2)\n",
    "* [3. Try to Control the Camera with Python Code](#Ch3)\n",
    "* [4. Create a Capture Booth GUI to Register Participants' Body Pictures](#Ch4)\n",
    "* [5. Capture Participants' Poses](#Ch5)\n",
    "  * [5.1 Classification](#Ch51)\n",
    "    * [5.1.1 Check if tensorflow works for image classification](#Ch511)\n",
    "    * [5.1.2 Check if the USB coral accelerator works for image classification](#Ch512)\n",
    "  * [5.2 PoseNet](#Ch52)\n",
    "    * [5.2.1 How does it work?](#Ch521)\n",
    "    * [5.2.2 Important PoseNet Concepts](#Ch522)\n",
    "    * [5.2.3 Example PoseNet Code](#Ch523)\n",
    "  * [5.3 Save Pose Data to a CSV](#Ch53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50c1b3",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup Hardware <a id=\"Ch1\"></a>\n",
    "\n",
    "\n",
    "### 1.1 Connect the Camera Module <a id=\"Ch11\"></a>\n",
    "\n",
    "<div>\n",
    "<img src=\"images/Camera_and_pi_4.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "Ensure your Raspberry Pi is turned off.\n",
    "\n",
    "1. Locate the Camera Module port\n",
    "\n",
    "<div>\n",
    "<img src=\"images/pi4-camera-port.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "2. Gently pull up on the edges of the port‚Äôs plastic clip\n",
    "\n",
    "<div>\n",
    "<img src=\"images/pull_edges.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "3. Insert the Camera Module ribbon cable; make sure the connectors at the bottom of the ribbon cable are facing the contacts in the port\n",
    "\n",
    "<div>\n",
    "<img src=\"images/facing_backwards.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "4. Push the plastic clip back into place\n",
    "\n",
    "\n",
    "### 1.2 Connect the USB Coral Accelerator <a id=\"Ch12\" />\n",
    "\n",
    "1. Make sure your RP is switched off.\n",
    "2. Plug the USB Coral Accelerator dongle into a **blue** (USB3) USB slot\n",
    "3. **Note**: the <span style=\"color:#0000ff\">blue</span> USB ports are faster than the not-blue ones\n",
    "\n",
    "\n",
    "### 1.3 Power Up the Pi  <a id=\"Ch13\" />\n",
    "\n",
    "1. Connect the USB-C _charger_ to the Pi\n",
    "2. The Pi will automatically switch on as soon as it has power\n",
    "3. You should now be able to connect to the Pi via VNC (if you are not connected with a physical external screen).\n",
    "\n",
    "\n",
    "## 2. Setup Software  <a id=\"Ch2\" />\n",
    "\n",
    "The lab organizers have already gone through [X0_SoftwareSetup](../X0_SoftwareSetup/README.md), which sets up the necessary software for you, before you were given the Pi, so you probably don't need to set up any software.\n",
    "\n",
    "The configuration script does things like enabling the Pi camera interface and installing `guizero`. If you're curious about what it did, you can read through `s1.py`'s source code in the [pbl](../X0_SoftwareSetup/pbl/pbl) module.\n",
    "\n",
    "> ‚ÑπÔ∏è **Problem With Your Pi?**\n",
    ">\n",
    "> The course organizers have tried their best to ensure all the configuration options and software you'll need is already installed before the course begins, but we can miss things. If you find that the Pi isn't working for you then you can try:\n",
    ">\n",
    "> - Asking for help\n",
    "> - Running `pbl test` in the terminal, which runs some basic checks that ensure things like libraries etc. are installed\n",
    "> - Reinstalling the necessary software by running `sudo pbl install` in the terminal (‚ö†Ô∏è **warning**: takes a long time)\n",
    "> - Manually going through the legacy setup guide [here](Legacy/S1_LegacySoftwareSetup.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f981b0c",
   "metadata": {},
   "source": [
    "## 3. Try to Control the Camera with Python Code <a id=\"Ch3\" />\n",
    "\n",
    "The Python `picamera` library allows you to control your Camera Module. \n",
    "1. Open a Python editor on the RP to start a new Python3 script\n",
    "2. **Save the script in a new folder: `home/pbl/Documents/Lab1/CAM_test.py`**\n",
    ". __‚ö†Ô∏è Warning:__ never save the file as `picamera.py`!\n",
    "3. Try the following code on your Pi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from picamera import PiCamera\n",
    "from time import sleep\n",
    "\n",
    "camera = PiCamera()\n",
    "\n",
    "camera.start_preview()\n",
    "sleep(5)\n",
    "camera.stop_preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49447e2c",
   "metadata": {},
   "source": [
    "Save and run this program. The camera preview should be shown for five seconds and then close again. \n",
    "\n",
    "> ‚ÑπÔ∏è **Note**: the camera preview only works when a monitor is directly connected to your Raspberry Pi. If you are using remote access (such as SSH or VNC), you won‚Äôt be able to see the camera preview. You can work around this by saving an image and viewing that instead (the next steps of this lab).\n",
    "\n",
    "> ‚ùì **Test Yourselves**: Try to describe line-by-line what this python code is doing.\n",
    "\n",
    "<br />\n",
    "\n",
    "> üèÜ **Challenge `S1.3`**: Adjust the `CAM_test.py` script to save a picture from the camera by using the `camera.capture()` function. Save the image as `capture.jpg` in a folder you make: **`home/pbl/Documents/Lab1/Captures`**\n",
    ">\n",
    "> ‚ÑπÔ∏è **Note**: it‚Äôs important to sleep for at least two seconds _before_ capturing an image, to give the camera time to adjust to the room's light levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b233702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to your code\n",
    "camera.capture(\"home/pbl/Documents/Lab1/Captures/capture.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252e6e5",
   "metadata": {},
   "source": [
    "If your picture is upside-down, you can rotate it by 180 degrees by adding the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b696d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.rotation = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc2310",
   "metadata": {},
   "source": [
    "You can rotate the image by 90, 180, or 270 degrees. To reset the image, set `camera.rotation` to 0 degrees.\n",
    "\n",
    "The Python `picamera` software provides a number of effects and configurations to change how your images look. Check out the following website to find some examples:\n",
    "https://projects.raspberrypi.org/en/projects/getting-started-with-picamera/7\n",
    "\n",
    "All documentation on the PiCamera project can be found here:\n",
    "https://picamera.readthedocs.io/en/release-1.13/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1245435",
   "metadata": {},
   "source": [
    "## 4. Create a Capture Booth GUI to Register Participants' Body Pictures <a id=\"Ch4\" />\n",
    "Now that you have gotten to know the picamera a bit better, you will now make a simple GUI that you can use to capture pictures of your participants' (clothed üòâ) bodies. You will do this in the following two challenges (üèÜ`S1.4a` and  üèÜ`S1.4b`). **Make a new script: `home/pbl/Documents/Lab1/CAM_register_participants.py`**. \n",
    "\n",
    "### 4.1 Step 1\n",
    "> üèÜ **Challenge `S1.4a`**: Use `guizero` (see [L3](../L3_PythonGUIsAndHardware/L3_PythonGUIsAndHardware.ipynb)) to create a Preview/Save Image GUI. \n",
    "> - Create two buttons: one button `Preview` and one button `Save image`: \n",
    "> - - The `Preview` button should cause the camera to show a preview\n",
    "> - - The `Save image` button should cause the camera to save an image (Tip: use the `camera.capture()` function again). Save the image in a folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581c7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the GUI described in Challenge S1.4a here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60beb2",
   "metadata": {},
   "source": [
    "### 4.2 Step 2 \n",
    "Now that you have made this GUI, you will improve it to create a **Capture Booth GUI to Register Participants' Body Pictures**.\n",
    "\n",
    "> üèÜ **Challenge `S1.4b`**: Use `guizero` (see [L3](../L3_PythonGUIsAndHardware/L3_PythonGUIsAndHardware.ipynb)) to create a capture booth GUI. \n",
    "> - The GUI should request a participant ID (e.g. via a text box in which you type an ID like  `P01`) that you can select\n",
    "> - The GUI should have a button that, when pressed, causes the application to take a picture of the participant's body\n",
    "> - The picture should be saved as a file with a relevant name (e.g. `P01_front.png`) in a folder called `Participants` (`home/pbl/Documents/Lab1/Participants_captures`). \n",
    "> - The GUI should show the captured picture in the GUI\n",
    "> - **The majority of this script is given in the box below. Read through the script carefully!**\n",
    "> - **Adjust the `capture()` function so that it saves the image in the correct location**\n",
    ">\n",
    "> ‚ÑπÔ∏è **Note**:\n",
    "> - The face of the participant should not be shown in the picture. Make sure that the camera only captures a picture of the body by verbally instructing the participant on where to stand in front of the camera (or move the camera around).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68282b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjust this code to create the GUI described in Challenge S1.4b here\n",
    "\n",
    "# Import packages\n",
    "from picamera import PiCamera\n",
    "from time import sleep\n",
    "from guizero import *\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "# Handling the participant ID is done using class_csv_handler_id \n",
    "# This is a custom-made class for this sensor, which was installed\n",
    "# previously on your Pi in /home/pbl/Desktop/\n",
    "import sys\n",
    "sys.path.insert(0, '/home/pbl/Desktop/')\n",
    "print(sys.path)\n",
    "from class_csv_handler_id import *\n",
    "\n",
    "# This creates the GUI with 3 boxes \n",
    "app = App(title=\"Capture Booth\", width=800, height=400)\n",
    "global camera\n",
    "camera = PiCamera()\n",
    "\n",
    "box1 = Box(app, align =\"left\", layout=\"auto\", width=300, height=350)\n",
    "box2 = Box(app, align =\"right\", width=500, height=350)\n",
    "box3 = Box(app, align =\"top\", width=300, height=50)\n",
    "\n",
    "# Code for registering a new participant ID\n",
    "# This function saves participant IDs that are provided through the GUI to a .csv file\n",
    "def ask_id(): \n",
    "    new_id = app.question(\"New participant\", \"Enter participant ID\")\n",
    "    if new_id is not None:\n",
    "        if [new_id] in id_list:\n",
    "            info(\"\",\"That participant is already registered\")\n",
    "\n",
    "        else:\n",
    "            new_id = new_id.split()\n",
    "            id_handler.write_id(\"/home/pbl/Documents/Lab1/Participants_captures/Participant_IDs.csv\", new_id)\n",
    "            id_list.append(new_id)\n",
    "            id_box.append(new_id)\n",
    "    return id_list\n",
    "\n",
    "# Code for selecting a participant ID\n",
    "# This function lets you select a participant ID from the box in the GUI\n",
    "def select_id(): \n",
    "    p_id = id_box.value[0]\n",
    "    print(p_id, type(p_id))\n",
    "    return p_id\n",
    "\n",
    "# Code for capturing an image using the Raspberry Pi camera\n",
    "def capture(): \n",
    "    p_id = select_id() #First, the participant ID should be selected on the GUI\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    # set this variable: \n",
    "    # capture_name = \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    picture = Picture(box2, image=capture_name, width=300, height=300)\n",
    "       \n",
    "# Creates a directory /home/pbl/Documents/Lab1/Participants_captures/ if it does not exist yet\n",
    "if not os.path.isdir(\"/home/pbl/Documents/Lab1/Participants_captures/\"):\n",
    "    os.mkdir(\"/home/pbl/Documents/Lab1/Participants_captures/\")\n",
    "    \n",
    "# Uses the class_csv_handler_id\n",
    "id_handler = csv_handler_id()\n",
    "id_list = id_handler.read_ids(\"/home/pbl/Documents/Lab1/Participants_captures/Participant_IDs.csv\")\n",
    "\n",
    "# Adds things to the GUI boxes\n",
    "Text(box1, text=\"Select participant\", size=11)\n",
    "id_box = ListBox(box1, id_list, command=select_id, scrollbar=True)\n",
    "\n",
    "button_new = PushButton(box1, text=\"Register new participant\", command=ask_id)\n",
    "button_cap = PushButton(box1, text=\"Capture!\", command=capture)\n",
    "\n",
    "# Shows the GUI\n",
    "app.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0fe52",
   "metadata": {},
   "source": [
    "## 5. Capture Participants' Poses <a id=\"Ch5\"></a>\n",
    "\n",
    "Pose estimation is the task of using an Machine Learning model to estimate the pose of a person from an image or a video by estimating the spatial locations of key body joints (keypoints). In this lab you are going to set up a __portable pose estimation lab__ using the __Raspberry Pi 4__, the __Pi Camera__, and the __Coral USB Accelerator__.\n",
    "\n",
    "The __Coral USB Accelerator__ is a USB device that provides an __Edge TPU__ as a coprocessor for your device. It accelerates inferencing for your machine learning models when attached to either a Linux, Mac, or Windows host computer. \n",
    "\n",
    "The pose estimation models take a processed camera image as the input and outputs information about keypoints. The keypoints detected are indexed by a part ID, with a confidence score between 0.0 and 1.0. The confidence score indicates the probability that a keypoint exists in that position.\n",
    "\n",
    "There are two TensorFlow Lite pose estimation models:\n",
    "- MoveNet: the state-of-the-art pose estimation model available in two flavors: Lighting and Thunder. \n",
    "- PoseNet: the previous generation pose estimation model released in 2017.\n",
    "\n",
    "In this lab you'll work with the __PoseNet__ model.\n",
    "\n",
    "The various body joints detected by the pose estimation model are tabulated below:\n",
    "\n",
    "\n",
    "| Id | Part |\n",
    "| --- | --- |\n",
    "|0|\tnose |\n",
    "|1|\tleftEye |\n",
    "|2|\trightEye |\n",
    "|3|\tleftEar |\n",
    "|4|\trightEar |\n",
    "|5|\tleftShoulder|\n",
    "|6|\trightShoulder|\n",
    "|7|\tleftElbow|\n",
    "|8|\trightElbow|\n",
    "|9|\tleftWrist|\n",
    "|10|\trightWrist|\n",
    "|11\t| leftHip|\n",
    "|12\t| rightHip|\n",
    "|13\t| leftKnee|\n",
    "|14\t| rightKnee|\n",
    "|15|\tleftAnkle|\n",
    "|16\t| rightAnkle|\n",
    "\n",
    "\n",
    "\n",
    "![PoseNetExample](images/PoseNet_example.png)\n",
    "\n",
    "source: https://www.tensorflow.org/lite/examples/pose_estimation/overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48715632",
   "metadata": {},
   "source": [
    "### 5.1 Classification <a class=\"anchor\" id=\"Ch51\"></a>\n",
    "\n",
    "We will first run general examples to check if the required libraries are installed. In this example we are going to classify the following image:\n",
    "\n",
    "<div>\n",
    "<img src=\"images/parrot.jpg\" width=\"200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2740790",
   "metadata": {},
   "source": [
    "#### 5.1.1 Check if tensorflow works for image classification <a id=\"Ch511\" />\n",
    "\n",
    "First we want to navigate to the folder containing our test, do this by typing in the Terminal:\n",
    "\n",
    "```bash\n",
    "cd /opt/coral_example\n",
    "```\n",
    "    \n",
    "Than we will try classify an example image with a tensorflow model, do this by copy-pasting the following code and press enter:\n",
    "\n",
    "```bash\n",
    "python3 classify_image.py \\\n",
    "  --model mobilenet_v2_1.0_224_inat_bird_quant.tflite \\\n",
    "  --labels inat_bird_labels.txt \\\n",
    "  --input parrot.jpg\n",
    "```\n",
    "\n",
    "You should get something like this:\n",
    "\n",
    "```text\n",
    "----INFERENCE TIME----\n",
    "Note: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.\n",
    "317.5ms\n",
    "288.7ms\n",
    "286.4ms\n",
    "286.4ms\n",
    "286.5ms\n",
    "-------RESULTS--------\n",
    "Ara macao (Scarlet Macaw): 0.77734\n",
    "```\n",
    "\n",
    "#### 5.1.2 Check if the USB coral accelerator works for image classification <a id=\"Ch512\" />\n",
    "\n",
    "Now we will check the availability of the USB coral accelerator by running the same test again. This time we use a model that is compiled to run on the USB tpu.\n",
    "\n",
    "Copy-paste the following code and run by pressing enter:\n",
    "\n",
    "```bash\n",
    "python3 classify_image.py \\\n",
    "  --model mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite \\\n",
    "  --labels inat_bird_labels.txt \\\n",
    "  --input parrot.jpg\n",
    "```\n",
    "    \n",
    "You should see a result like this:\n",
    "\n",
    "```text\n",
    "----INFERENCE TIME----\n",
    "Note: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.\n",
    "11.8ms\n",
    "3.0ms\n",
    "2.8ms\n",
    "2.9ms\n",
    "2.9ms\n",
    "-------RESULTS--------\n",
    "Ara macao (Scarlet Macaw): 0.75781\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8c8c8",
   "metadata": {},
   "source": [
    "### 5.2 PoseNet <a id=\"Ch52\" />\n",
    "\n",
    "(https://github.com/google-coral/project-posenet)\n",
    "\n",
    "#### 5.2.1 How does it work? <a id=\"Ch521\" />\n",
    "\n",
    "At a high level, pose estimation happens in two phases:\n",
    "\n",
    "An input RGB image is fed through a convolutional neural network. In our case this is a MobileNet V1 architecture. Instead of a classification head however, there is a specialized head which produces a set of heatmaps (one for each kind of key point) and some offset maps. This step runs on the EdgeTPU. The results are then fed into step 2)\n",
    "\n",
    "- A special multi-pose decoding algorithm is used to decode poses, pose confidence scores, keypoint positions, and keypoint confidence scores. \n",
    "\n",
    "- If you're interested in the details of the decoding algorithm and how PoseNet works under the hood, you could take a look at the original research paper or this post: https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5 which describes the raw heatmaps produced by the convolutional model.\n",
    "\n",
    "#### 5.2.2 Important PoseNet Concepts <a id=\"Ch522\" />\n",
    "\n",
    "<div> <img src=\"images/keypoints.png\" width=\"800\"></div>\n",
    "\n",
    "| Concept | Description |\n",
    "| ------- | ----------- |\n",
    "| Pose    | At the highest level, PoseNet will return a pose object that contains a list of keypoints and an instance-level confidence score for each detected person. |\n",
    "| Keypoint | A part of a person‚Äôs pose that is estimated, such as the nose, right ear, left knee, right foot, etc. It contains both a position and a keypoint confidence score. PoseNet currently detects 17 keypoints illustrated in the diagram above.\n",
    "| Keypoint Confidence Score | This determines the confidence that an estimated keypoint position is accurate. It ranges between 0.0 and 1.0. It can be used to hide keypoints that are not deemed strong enough. |\n",
    "| Keypoint Position | 2D x and y coordinates in the original input image where a keypoint has been detected. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac0893",
   "metadata": {},
   "source": [
    "#### 5.2.3 Example PoseNet Code <a id=\"Ch523\" />\n",
    "\n",
    "> ‚ÑπÔ∏è **Note**: PoseNet should already have been installed for you\n",
    ">\n",
    "> This is explained in the Software Setup section, but if you are having issues then you can also try to manually go through the legacy notes [here](Legacy/S1_LegacySoftwareSetup.ipynb)\n",
    "\n",
    "The example code, `pose_camera.py`, is a camera example that streams the camera's image through posenet and draws the pose on top as an overlay. This is a great first example to run to familiarize yourself with the network and its outputs.\n",
    "\n",
    "Run the demo in a terminal:\n",
    "\n",
    "```bash\n",
    "cd /opt/project-posenet\n",
    "python3 pose_camera.py\n",
    "```\n",
    "\n",
    "If the camera and monitor are both facing you, consider adding the `--mirror` flag:\n",
    "\n",
    "```bash\n",
    "python3 pose_camera.py --mirror\n",
    "```\n",
    "\n",
    "> ‚ÑπÔ∏è **Note**: The github repository (https://github.com/google-coral/project-posenet.git) contains the following 3 posenet model files in `models/mobilenet` for different input resolutions. The larger resolutions process more slowly, but allow a wider field of view, for further-away poses to be processed correctly.\n",
    ">\n",
    "> ```text\n",
    "> posenet_mobilenet_v1_075_721_1281_quant_decoder_edgetpu.tflite\n",
    "> posenet_mobilenet_v1_075_481_641_quant_decoder_edgetpu.tflite\n",
    "> posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite\n",
    "> ```\n",
    "\n",
    "You can change the camera resolution by using the --res parameter:\n",
    "\n",
    "```bash\n",
    "python3 pose_camera.py --res 480x360  # fast but low res\n",
    "python3 pose_camera.py --res 640x480  # default\n",
    "python3 pose_camera.py --res 1280x720 # slower but high res\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6463c43",
   "metadata": {},
   "source": [
    "### 5.3 Save Pose Data to a CSV <a id=\"Ch53\" />\n",
    "In the previous section you have extracted keypoints from a live video using PoseNet. However, we cannot analyse data if this isn't saved somewhere.\n",
    "\n",
    "To do this, you are going to need to know how to generate unique timestamped filenames ([X2](../X2_GeneratingTimestampedFilenames/X2_GeneratingTimestampedFilenames.ipynb)) and how to write to CSV files ([X1](../X1_WritingCSVFiles/X1_WritingCSVFiles.ipynb)). **You will then produce a new python script, `home/pbl/Desktop/project-posenet/CAM_logging_data.py`, that writes your keypoint values (and a timestamp) to a CSV file.**\n",
    "\n",
    "> üèÜ **Challenge `S1.5.3a`**: Go through the [X1](../X1_WritingCSVFiles/X1_WritingCSVFiles.ipynb) and [X2](../X2_GeneratingTimestampedFilenames/X2_GeneratingTimestampedFilenames.ipynb) \"eXtra Content\" materials. \n",
    ">\n",
    "> - After going through [X1](../X1_WritingCSVFiles/X1_WritingCSVFiles.ipynb), you should know how to write CSV files\n",
    "> - After going through [X2](../X2_GeneratingTimestampedFilenames/X2_GeneratingTimestampedFilenames.ipynb), you should know how to generate timestamped file names\n",
    "> - Combine both techniques to write your data to a *timestamped* CSV file\n",
    ">\n",
    ">\n",
    "> üèÜ **Challenge `S2.5.2b`**: **Create a new script: \n",
    "`CAM_logging_data.py`**. This script should create a timestamped CSV file containing the raw data. \n",
    "> 1. First, verify that you can run the `pose_camera.py` script as was done in the terminal using: \n",
    "> ```bash\n",
    "> cd /opt/project-posenet\n",
    "> python3 pose_camera.py\n",
    "> ```\n",
    "> 2. Make a copy of `/opt/project-posenet` on your desktop with `cp -ar /opt/project-posenet ~/Desktop`\n",
    "> 3. Verify that you can run this `pose_camera.py` script using: \n",
    "> ```bash\n",
    "> cd ~/Desktop/project-posenet\n",
    "> python3 pose_camera.py\n",
    "> ```\n",
    "> Create a script in the new folder: **`home/pbl/Desktop/project-posenet/CAM_logging_data.py`**\n",
    "> - Use the script that is provided below to start the `CAM_logging_data.py` script. \n",
    "> - Adjust the script to generate a timestamped CSV filename (e.g. `$yourpath$/output_$timestamp$.csv`). Create a list (or other data type) and append a row of keypoint data each time new datapoints are generated by the loop. \n",
    "> - Make sure all pose estimation datapoints are written to the CSV file\n",
    "> - Make sure that when the program stops, you **close** the CSV file\n",
    ">\n",
    ">\n",
    "> üí° **Tips**:\n",
    ">\n",
    "> - You only need one file (via `open`) and one `csv.writer` for the entire acquisition.\n",
    "> - In the code below, it is indicated where you should add code\n",
    "> - In the last cell of this notebook, an example is given for the loop that you should write\n",
    "> - You will need to insert multiple rows during an acquisition - one per frame. \n",
    "> - `render_overlay` is called once per frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Desktop/project-posenet/')\n",
    "from pose_camera import *\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "n = 0\n",
    "sum_process_time = 0\n",
    "sum_inference_time = 0\n",
    "ctr = 0\n",
    "fps_counter = avg_fps_counter(30)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "# Create filename and open the file\n",
    "# Create csvwriter\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# USE THIS PIECE OF CODE TO GET THE NAMES (KEYS) OF THE KEYPOINTS   \n",
    "# listkeys = [element for tupl in EDGES for element in tupl]\n",
    "# listkeys = list(set(listkeys))\n",
    "# listkeys = [str(p) for p in listkeys]\n",
    "# listX = [s + '_x' for s in listkeys]\n",
    "# listY = [s + '_y' for s in listkeys]\n",
    "# xykeyslist = listX + listY\n",
    "# USE THIS PIECE OF CODE TO GET THE NAMES (KEYS) OF THE KEYPOINTS   \n",
    "\n",
    "### YOUR CODE HERE\n",
    "# Write a toprow to your csv file that contains 'date' and all keys\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def run_inference(engine, input_tensor):\n",
    "    return engine.run_inference(input_tensor)\n",
    "\n",
    "def render_overlay(engine, output, src_size, inference_box):\n",
    "    global n, sum_process_time, sum_inference_time, fps_counter\n",
    "\n",
    "    svg_canvas = svgwrite.Drawing('', size=src_size)\n",
    "    start_time = time.monotonic()\n",
    "    outputs, inference_time = engine.ParseOutput()\n",
    "    end_time = time.monotonic()\n",
    "    n += 1\n",
    "    sum_process_time += 1000 * (end_time - start_time)\n",
    "    sum_inference_time += inference_time * 1000\n",
    "\n",
    "    avg_inference_time = sum_inference_time / n\n",
    "    text_line = 'PoseNet: %.1fms (%.2f fps) TrueFPS: %.2f Nposes %d' % (\n",
    "        avg_inference_time, 1000 / avg_inference_time, next(fps_counter), len(outputs)\n",
    "    )\n",
    "\n",
    "    shadow_text(svg_canvas, 10, 20, text_line)\n",
    "    for pose in outputs:\n",
    "        \n",
    "        draw_pose(svg_canvas, pose, src_size, inference_box)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # Make empty lists \n",
    "        # Create loop: for i in pose.keypoints: and append keypoints for x an y\n",
    "        # Add xpoints to ypoints and insert a datestamp\n",
    "        # Write the row to csv\n",
    "        ### YOUR CODE HERE\n",
    "    \n",
    "    return (svg_canvas.tostring(), False)\n",
    "\n",
    "run(run_inference, render_overlay)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "# Close the file here\n",
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ede698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí° tip: you may need to build a row of your CSV cell-by-cell\n",
    "from datetime import datetime\n",
    "\n",
    "xpoints = []\n",
    "ypoints = []\n",
    "for i in pose.keypoints: # loop through all keypoints\n",
    "    xpoints.append(pose.keypoints[i].point[0]) # append the x-coordinate of the keypoint to the row \n",
    "    ypoints.append(pose.keypoints[i].point[1]) # append the y-coordinate of the keypoint in the row\n",
    "\n",
    "xypoints = xpoints + ypoints\n",
    "xypoins.insert(0, str(datetime.now())) # insert a timestamp in the first column \n",
    "\n",
    "# (and then you need to write this row to a CSV using a `csv.writer`: see X1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
