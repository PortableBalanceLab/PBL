{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d6bf27",
   "metadata": {},
   "source": [
    "# PBL Lab 1: portable pose estimation lab\n",
    "\n",
    "Pose estimation refers to computer vision techniques that detect human figures in images and videos, so that one could determine, for example, where someone’s elbow shows up in an image. It is important to be aware of the fact that pose estimation merely estimates where key body joints are and does not recognize who is in an image or video. In this lab we will be working with the [Raspberry Pi 4](https://www.raspberrypi.com/products/raspberry-pi-4-model-b/), the [Pi Camera](https://projects.raspberrypi.org/en/projects/getting-started-with-picamera), and a [Coral USB Accelerator](https://coral.ai/products/accelerator/).\n",
    "\n",
    "__Outline:__\n",
    "* [1. Connect the camera Module](#Ch1)\n",
    "* [2. Controlling the Camera with Python code](#Ch2)\n",
    "* [3. Creating a Capture Booth to Register participants' pictures](#Ch3)\n",
    "* [4.  Pose estimation](#Ch4)\n",
    "    * [4.1. Classification](#Ch41)\n",
    "    * [4.2. PoseNet](#Ch42)\n",
    "    * [4.3. Save data to a file](#Ch43)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50c1b3",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Connect the camera Module <a class=\"anchor\" id=\"Ch1\"></a>\n",
    "\n",
    "<div>\n",
    "<img src=\"images/Camera_and_pi_4.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "Ensure your Raspberry Pi is turned off.\n",
    "1. Locate the Camera Module port\n",
    "<div>\n",
    "<img src=\"images/pi4-camera-port.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "2. Gently pull up on the edges of the port’s plastic clip\n",
    "<div>\n",
    "<img src=\"images/pull_edges.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "3. Insert the Camera Module ribbon cable; make sure the connectors at the bottom of the ribbon cable are facing the contacts in the port\n",
    "<div>\n",
    "<img src=\"images/facing_backwards.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "4. Push the plastic clip back into place\n",
    "\n",
    "5. Start up your Raspberry Pi\n",
    "6. Go to the main menu and open the __Raspberry Pi Configuration__ tool.\n",
    "7. Select the __Interfaces__ tab and ensure that the __camera__ is __enabled__\n",
    "8. Reboot your Raspberry Pi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f981b0c",
   "metadata": {},
   "source": [
    "## 2. Controlling the Camera with Python code <a class=\"anchor\" id=\"Ch2\"></a>\n",
    "The Python __picamera__ library allows you to control your Camera Module. \n",
    "1. Open a new file in the editor (e.g. in Mu) and save it as __camera_example.py__\n",
    "__Note:__ never save the file as __picamera.py__!\n",
    "\n",
    "2. Try the following code on your RP:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from picamera import PiCamera\n",
    "from time import sleep\n",
    "\n",
    "camera = PiCamera()\n",
    "\n",
    "camera.start_preview()\n",
    "sleep(5)\n",
    "camera.stop_preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49447e2c",
   "metadata": {},
   "source": [
    "__Test yourselves: Try to describe line by line what is happening__\n",
    "\n",
    "Save and run this program. The camera preview should be shown for five seconds and then close again. \n",
    "\n",
    "__Note:__ the camera preview only works when a monitor is connected to your Raspberry Pi. If you are using remote access (such as SSH or VNC), you won’t’ see the camera preview. So instead, let's save the picture using __camera.capture()__. \n",
    "- save the the image as 'capture01.jpg'in the directory '/home/pi/Desktop'\n",
    "- It’s important to sleep for at least two seconds _before_ capturing an image, because this gives the camera’s sensor time to sense the light levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b233702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your own code here #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252e6e5",
   "metadata": {},
   "source": [
    "If your picture is upside-down, you can rotate it by 180 degrees by adding the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b696d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.rotation = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc2310",
   "metadata": {},
   "source": [
    "You can rotate the image by 90, 180, or 270 degrees. To reset the image, set rotation to 0 degrees.\n",
    "\n",
    "The Python picamera software provides a number of effects and configurations to change how your images look. Check out the following website to find some examples:\n",
    "https://projects.raspberrypi.org/en/projects/getting-started-with-picamera/7\n",
    "\n",
    "All documentation on the PiCamera project can be found here:\n",
    "https://picamera.readthedocs.io/en/release-1.13/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60beb2",
   "metadata": {},
   "source": [
    "## 3. Creating a Capture Booth to Register participants' pictures. <a class=\"anchor\" id=\"Ch3\"></a>\n",
    "\n",
    "Now that you got to know the picamera a bit better, make a simple interface that you can use to capture pictures of your participants. The GUI should be able to do the following:\n",
    "\n",
    "- enter participant label(e.g.\"P01\")\n",
    "- Have a button to take a picture from the front and automatically saves the picture (\"participant_number + front\") in folder 'participants'\n",
    "- put a text overlay \"participant_number\" over the pictures \n",
    "- show the picture when taken\n",
    "\n",
    "Tip:\n",
    "-First create your lay-out without functionality; use a placeholder image at the position where the captured image should be placed\n",
    "\n",
    "\n",
    "__Note!__ you do not want the face of the participant to be recogizable, so make sure that the picture only captures the body by verbally instructing the participant on where to stand in front of the camera (or move the camera around)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68282b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out your own code here #\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0fe52",
   "metadata": {},
   "source": [
    "## 4. Pose estimation <a class=\"anchor\" id=\"Ch4\"></a>\n",
    "\n",
    "Pose estimation is the task of using an Machine Learning model to estimate the pose of a person from an image or a video by estimating the spatial locations of key body joints (keypoints). In this lab you are going to set up a __portable pose estimation lab__ using the __Raspberry Pi 4__, the __Pi Camera__, and the __Coral USB Accelerator__. The __Coral USB Accelerator__ is a USB device that provides an __Edge TPU__ as a coprocessor for your device. It accelerates inferencing for your machine learning models when attached to either a Linux, Mac, or Windows host computer. \n",
    "\n",
    "The pose estimation models take a processed camera image as the input and outputs information about keypoints. The keypoints detected are indexed by a part ID, with a confidence score between 0.0 and 1.0. The confidence score indicates the probability that a keypoint exists in that position.\n",
    "\n",
    "There are two TensorFlow Lite pose estimation models:\n",
    "- MoveNet: the state-of-the-art pose estimation model available in two flavors: Lighting and Thunder. \n",
    "- PoseNet: the previous generation pose estimation model released in 2017.\n",
    "\n",
    "In this lab you'll work with the __PoseNet__ model.\n",
    "\n",
    "The various body joints detected by the pose estimation model are tabulated below:\n",
    "\n",
    "\n",
    "| Id | Part |\n",
    "| --- | --- |\n",
    "|0|\tnose |\n",
    "|1|\tleftEye |\n",
    "|2|\trightEye |\n",
    "|3|\tleftEar |\n",
    "|4|\trightEar |\n",
    "|5|\tleftShoulder|\n",
    "|6|\trightShoulder|\n",
    "|7|\tleftElbow|\n",
    "|8|\trightElbow|\n",
    "|9|\tleftWrist|\n",
    "|10|\trightWrist|\n",
    "|11\t| leftHip|\n",
    "|12\t| rightHip|\n",
    "|13\t| leftKnee|\n",
    "|14\t| rightKnee|\n",
    "|15|\tleftAnkle|\n",
    "|16\t| rightAnkle|\n",
    "\n",
    "\n",
    "\n",
    "![PoseNetExample](images/PoseNet_example.png)\n",
    "[source: https://www.tensorflow.org/lite/examples/pose_estimation/overview]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48715632",
   "metadata": {},
   "source": [
    "### 4.1 Classification <a class=\"anchor\" id=\"Ch41\"></a>\n",
    "\n",
    "We will first run general examples to  check if the required libraries are installed. In this example we are going to classify the following image:\n",
    "\n",
    "<div>\n",
    "<img src=\"images/parrot.jpg\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "\n",
    "1. Make sure your RP is switched off.\n",
    "2. Connect the RP to:\n",
    "    - Pi Camera\n",
    "    - USB Coral Accelerator\n",
    "3. Now connect the _charger_ to switch on the Raspberry Pi\n",
    "4. Connect with the RP via de VNC Viewer (if you are not connected to an external screen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2740790",
   "metadata": {},
   "source": [
    "#### 4.1.1 Check if tensorflow works for image classification:\n",
    "First we want to navigate to the folder containing our test, do this by typing in the Terminal:\n",
    "\n",
    "    cd coral/tflite/python/examples/classification\n",
    "    \n",
    "Than we will try classify an example image with a tensorflow model, do this by copy-pasting the following code and press enter:\n",
    "\n",
    "    python3 classify_image.py \\\n",
    "    --model models/mobilenet_v2_1.0_224_inat_bird_quant.tflite \\\n",
    "    --labels models/inat_bird_labels.txt \\\n",
    "    --input images/parrot.jpg\n",
    "\n",
    "You should get something like this:\n",
    "\n",
    "        ----INFERENCE TIME----\n",
    "        Note: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.\n",
    "        317.5ms\n",
    "        288.7ms\n",
    "        286.4ms\n",
    "        286.4ms\n",
    "        286.5ms\n",
    "        -------RESULTS--------\n",
    "        Ara macao (Scarlet Macaw): 0.77734\n",
    "\n",
    "#### 4.1.2 Check if the USB coral accelerator works for image classification:\n",
    "Now we will check the availability of the USB coral accelerator by running the same test again. This time we use a model that is compiled to run on the USB tpu. Copy-paste the following code and run by pressing enter:\n",
    "\n",
    "    python3 classify_image.py \\\n",
    "    --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite \\\n",
    "    --labels models/inat_bird_labels.txt \\\n",
    "    --input images/parrot.jpg\n",
    "    \n",
    "You should see a result like this:\n",
    "\n",
    "    ----INFERENCE TIME----\n",
    "    Note: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.\n",
    "    11.8ms\n",
    "    3.0ms\n",
    "    2.8ms\n",
    "    2.9ms\n",
    "    2.9ms\n",
    "    -------RESULTS--------\n",
    "    Ara macao (Scarlet Macaw): 0.75781"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8c8c8",
   "metadata": {},
   "source": [
    "### 4.2 PoseNet <a class=\"anchor\" id=\"Ch42\"></a>\n",
    "\n",
    "(https://github.com/google-coral/project-posenet)\n",
    "\n",
    "#### 4.2.1 How does it work? <a class=\"anchor\" id=\"Ch421\"></a>\n",
    "At a high level pose estimation happens in two phases:\n",
    "\n",
    "An input RGB image is fed through a convolutional neural network. In our case this is a MobileNet V1 architecture. Instead of a classification head however, there is a specialized head which produces a set of heatmaps (one for each kind of key point) and some offset maps. This step runs on the EdgeTPU. The results are then fed into step 2)\n",
    "\n",
    "- A special multi-pose decoding algorithm is used to decode poses, pose confidence scores, keypoint positions, and keypoint confidence scores. \n",
    "\n",
    "- If you're interested in the details of the decoding algorithm and how PoseNet works under the hood, you could take a look at the original research paper or this post: https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5 which describes the raw heatmaps produced by the convolutional model.\n",
    "\n",
    "#### 4.2.2 Important concepts <a class=\"anchor\" id=\"Ch422\"></a>\n",
    "__Pose__: at the highest level, PoseNet will return a pose object that contains a list of keypoints and an instance-level confidence score for each detected person.\n",
    "\n",
    "__Keypoint__: a part of a person’s pose that is estimated, such as the nose, right ear, left knee, right foot, etc. It contains both a position and a keypoint confidence score. PoseNet currently detects 17 keypoints illustrated in the following diagram:\n",
    "\n",
    "<div> <img src=\"images/keypoints.png\" width=\"800\"></div>\n",
    "\n",
    "__Keypoint Confidence Score__: this determines the confidence that an estimated keypoint position is accurate. It ranges between 0.0 and 1.0. It can be used to hide keypoints that are not deemed strong enough.\n",
    "\n",
    "__Keypoint Position__: 2D x and y coordinates in the original input image where a keypoint has been detected.\n",
    "\n",
    "Now that we know that our TensorFlow and Coral are working, we will now install the required packages for PoseNet\n",
    "\n",
    "    sudo apt-get install python3-pycoral\n",
    "\n",
    "Install tflite:\n",
    "    \n",
    "    python3 -m pip install tflite-runtime\n",
    "\n",
    "Install the Edge TPU runtime:\n",
    "\n",
    "    echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "\n",
    "    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "\n",
    "    sudo apt-get update\n",
    "    \n",
    "    \n",
    "    sudo apt-get install libedgetpu1-std\n",
    "    \n",
    "\n",
    "Now it’s time to connect your __Coral__ using the USB-C cable supplied. This conforms to USB 3.0 standards and should be attached to one of the Blue USB ports on the Raspberry Pi to allow the fastest transfer speeds. If you already attached the Coral, then remove and connect again.\n",
    "     \n",
    "    cd ~/google-coral\n",
    "    \n",
    "try:\n",
    "\n",
    "    git clone https://github.com/google-coral/project-posenet.git\n",
    "    \n",
    "if you receive the message 'fatal: destination path 'project-posenet' already exists and is not an empty directory.' Posenet was already downloaded on your device. Move on to the next step:\n",
    "\n",
    "    cd /home/pi/google-coral/project-posenet\n",
    "    \n",
    "    sh install_requirements.sh\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac0893",
   "metadata": {},
   "source": [
    "#### 4.2.3 Example <a class=\"anchor\" id=\"Ch423\"></a>\n",
    "The example code _pose_camera.py_ is a camera example that streams the camera image through posenet and draws the pose on top as an overlay. This is a great first example to run to familiarize yourself with the network and its outputs.\n",
    "\n",
    "Run the demo like this:\n",
    "\n",
    "    cd /home/pi/google-coral/project-posenet\n",
    "\n",
    "    python3 pose_camera.py\n",
    "\n",
    "If the camera and monitor are both facing you, consider adding the --mirror flag:\n",
    "\n",
    "    python3 pose_camera.py --mirror\n",
    "\n",
    "\n",
    "_Optional:_ \\\n",
    "In the repo there are included 3 posenet model files for differnet input resolutions. The larger resolutions are slower of course, but allow a wider field of view, or further-away poses to be processed correctly.\n",
    "\n",
    "    posenet_mobilenet_v1_075_721_1281_quant_decoder_edgetpu.tflite\n",
    "    posenet_mobilenet_v1_075_481_641_quant_decoder_edgetpu.tflite\n",
    "    posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite\n",
    "\n",
    "You can change the camera resolution by using the --res parameter:\n",
    "\n",
    "    python3 pose_camera.py --res 480x360  # fast but low res\n",
    "    python3 pose_camera.py --res 640x480  # default\n",
    "    python3 pose_camera.py --res 1280x720 # slower but high res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6463c43",
   "metadata": {},
   "source": [
    "### 4.3 Saving data to file <a class=\"anchor\" id=\"Ch43\"></a>\n",
    "\n",
    "In the previous section you have extracted keypoints from a live video using OpenPose. However, we cannot analyse data if this isn't saved. You are now going to edit the '_pose_camera.py_' script such that the keypoints are written to a csv file.\n",
    "\n",
    "1. Make sure that you save the pose_camera.py under a different name: logging_pose_camera.py\n",
    "\n",
    "The first thing we need when we want to save experimental data is a timestamp for each keypoint. And, while we are on it, it would be great if the data are automatically saved with a filename that holds the date and time.\n",
    "\n",
    "2. Therefore, we need to import __datetime__ from the library __datetime__\n",
    "\n",
    "To explore what __datetime__ does, you can run the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "x = datetime.now()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e807eb",
   "metadata": {},
   "source": [
    "The date contains year, month, day, hour, minute, second, and microsecond. The __datetime__ module has many methods to return information about the date object.\n",
    "\n",
    "| Directive |\tDescription |\n",
    "| --- | --- |\n",
    "| %a\t| Weekday, short version\tWed\t|\n",
    "| %A\t| Weekday, full version\tWednesday\t| \n",
    "| %w\t| Weekday as a number 0-6, 0 is Sunday\t3| \t\n",
    "| %d\t| Day of month 01-31\t31\t| \n",
    "| %b\t| Month name, short version\tDec | \t\n",
    "| %B\t| Month name, full version\tDecember | \t\n",
    "| %m\t| Month as a number 01-12\t12\t| \n",
    "| %y\t| Year, short version, without century\t18\t | \n",
    "| %Y\t| Year, full version\t2018\t| \n",
    "| %H\t| Hour 00-23\t17\t| \n",
    "| %I\t| Hour 00-12\t05\t| \n",
    "| %p\t| AM/PM\tPM\t| \n",
    "| %M\t| Minute 00-59\t41\t| \n",
    "| %S\t| Second 00-59\t08\t| \n",
    "| %f\t| Microsecond 000000-999999\t548513\t| \n",
    "| %z\t| UTC offset\t+0100\t| \n",
    "| %Z\t| Timezone\tCST\t| \n",
    "| %j\t| Day number of year 001-366\t365\t| \n",
    "| %U\t| Week number of year, Sunday as the first day of week, 00-53\t52\t| \n",
    "| %W\t| Week number of year, Monday as the first day of week, 00-53\t52\t| \n",
    "| %c\t| Local version of date and time\tMon Dec 31 17:41:00 2018\t| \n",
    "| %C\t| Century\t20\t| \n",
    "| %x\t| Local version of date\t12/31/18\t| \n",
    "| %X\t| Local version of time\t17:41:00\t| \n",
    "| %%\t| A % character\t%\t| \n",
    "| %G\t| ISO 8601 year\t2018\t| \n",
    "| %u\t| ISO 8601 weekday (1-7)\t1 | \t\n",
    "| %V\t| ISO 8601 weeknumber (01-53)\t01\t |\n",
    "_(source: https://www.w3schools.com/python/python_datetime.asp )_\n",
    "\n",
    "3. Create a code to extract a string similar to: _20220404-114200_ (year,month,day - hour,minute,second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ddb73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "# Enter your own code here!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6636a0",
   "metadata": {},
   "source": [
    "4. Use the code you just wrote in the '__def__ main()' of your _logging_pose_camera.py_ script to create a filename _data/output_yourdatestring.csv_. Don't forget to also import the library at the start of your script.\n",
    "\n",
    "You will need the __datetime__ module again later on, but for now let's focus on creating a file in which we can log the data.\n",
    "\n",
    "5. In Lecture 3, you have learnt how to open a file with a specific filename. Use this to open a file in which we can _write_ with the filename you created in (4). Place this code also in the '__def__ main()' function.\n",
    "\n",
    "Instead of opening a text file like we did in Lecture 3 we now want to create a csv file. There is a special Python module we can use: __csv__\n",
    "\n",
    "6. import the csv module in _logging_pose_camera.py_\n",
    "\n",
    "csv.writer class is used to insert data to the CSV file. This class returns a writer object which is responsible for converting the user’s data into a delimited string. A csvfile object should be opened with _newline=''_ otherwise newline characters inside the quoted fields will not be interpreted correctly. Therefore, we need to slightly adjust the code we put in (5)\n",
    "\n",
    "7. add _newline=''_ within the brackets of the line in which you open the csv file. \n",
    "\n",
    "csv.writer class provides two methods for writing to CSV. They are __writerow()__ and __writerows()__. writerow() writes a single row at a time. writerow(fields) is used to write multiple rows at a time. Below is an example of how to write data into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ede698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Example\n",
    "\n",
    "import csv \n",
    "    \n",
    "# field names \n",
    "fields = ['Name', 'Course', 'Year', 'Grade'] \n",
    "    \n",
    "# data rows of csv file \n",
    "rows = [ ['Nikhil', 'KT2502', '2022', '9.0'], \n",
    "         ['Sanchit', 'KT2501', '2021', '7.1'], \n",
    "         ['Aditya', 'KT2502', '2022', '9.3'], \n",
    "         ['Sagar', 'KT2502', '2022', '9.5'], \n",
    "         ['Prateek', 'KT2502', '2022', '7.8'], \n",
    "         ['Sahil', 'KT2502', '2022', '9.1']] \n",
    "\n",
    "\n",
    "    \n",
    "# name of csv file \n",
    "filename = \"course_records.csv\"\n",
    "    \n",
    "# writing to csv file \n",
    "csvfile = open(filename, 'w',newline='') \n",
    "    \n",
    "# creating a csv writer object \n",
    "csvwriter = csv.writer(csvfile) \n",
    "        \n",
    "# writing the fields \n",
    "csvwriter.writerow(fields) \n",
    "        \n",
    "# writing the data rows \n",
    "csvwriter.writerows(rows)\n",
    "    \n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087e114",
   "metadata": {},
   "source": [
    "The file is saved in the same folder as this python notebook.\n",
    "\n",
    "7. Use this example to create a csv writer object in __def__ main(). Think about how you would like to export the keypoints, what will be your headers? Create this header (fields) and write it to the csv file.\n",
    "\n",
    "The rows will be filled with the keypoint data while you are measuring. This will be added to the  '__def__ render_overlay' function\n",
    "\n",
    "8. As an example, the following code adds the data to a single row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = []\n",
    "for label, keypoint in pose.keypoints.items(): #loops through all keypoints\n",
    "    row.append(keypoint.point[0]) # appends the x-coordinate of the keypoint to the row \n",
    "    row.append(keypoint.point[1]) # appends the y-coordinate of the keypoint in the row\n",
    "           \n",
    "# add date to row\n",
    "row.insert(0,now) # enters the date to as first entry of the row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749daa8b",
   "metadata": {},
   "source": [
    "      \n",
    "9. Write the data array that you constructed to the csv file.\n",
    "\n",
    "10. Finally, make sure that at the end of the __def__ main() function, you close the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0843041",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add your code to the Raspberry Pi Code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
